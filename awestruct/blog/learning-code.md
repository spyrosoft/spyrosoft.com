---
title: Learning Code
layout: blog
date: 5-4-16
---
Having spent time in both:

1. Academia on a course through computer science
2. The industry researching solutions to problems

I can personally attest to the difference between the way I want to be taught code and the way code tends to be taught.

### Academia

In academia a number of [antipatterns](https://en.wikipedia.org/wiki/Anti-pattern) arise.

The biggest, I think, is the notion of plagiarism. In an attempt to assess the abilities of students each student is required to produce independent work. This flies in the face of everything that is code. Reinventing the wheel when writing code can have dire consequences. Not only for encryption and security, it is also a habit which ultimately needs to be unlearned to optimize coding productivity. Open source is a real thing. The odds are that not only has someone else already written the (base) code needed, they have done a better job that the employee could ever do on their own on company time.

Another is the tendency to focus on the abstract over the practical. Don't get me wrong, the abstract is very important (and awesome). However without combining it with the practical it is then left up to employers to teach people how to code after graduation. Isn't the point of going to school to learn the skills required to perform a job/career? That was the impression that I had when I entered college. This opinion is shared with a friend of mine who graduated with a BS in computer science and now feels inadequate and afraid to even apply to jobs. In general I see the successful computer science graduates leave school leaning heavily on the skills that they have taught themselves over the years based on what they found they wanted to create for themselves.

Assignments are not like what real problems are like. Writing code is one giant endless collection of puzzles. Solving the puzzles (well) is an art. Learning this art from school assignments is difficult. Not always - some teachers have a knack for creating assignments that aim to do so. Thank your lucky stars if your teacher is this way.

There is a lack of focus on teaching best practices. For example naming convention, modularity of code, neatness, organization, etc. The classroom seems like an excellent opportunity to spread best practices. People tend to leave and use what they know so whatever they learned picks up enormous momentum. Instructors have the ability to research what the best utilities are for a given task (IDEs, languages, libraries, etc.) and share this knowledge with a massive number of new coders. Equipped with this knowledge coders will cease shoving round pegs into square holes.

Topic focus tends not to reflect the needs of the industry. This is obviously a tall order and I'm not sure how to solve it other than creating a nationwide collaborative curriculum (like that could ever happen). There is a misconception that the tech industry is constantly changing. It is certainly true in some respects, but much of it is an illusion. There are year shifts that I have noticed spanning four or so years which should be matched by what is taught. Even then, these shifts don't start and stop suddenly; there is a gradual lead-in and lead-out. Teaching people something which is on its way out is still valuable in my opinion and the focus should be on matching the shifts as well as is reasonable.

### Industry

In the quote unquote "Real World", much of learning how to code is reading through documentation in order to figure out what can be done at all. Ideally also learning what ways things can be done. Documentation is almost always in alphabetical order. Alphabetical order is great to use as a reference once the coder already knows what can be done. However it completely randomizes the order of both the usefulness and the frequency of use. For those of you who write references, please take this into serious consideration.

### Solution

Mentorship is a good temporary solution. It may also be a long term solution but the underlying issues need to be solved as well. A pay-it-forward mentality will do wonders for the computing industry. So much of writing code is the ability to foresee pitfalls and navigate around them. This is a skill that can be learned in two ways:

1. The hard way.
2. From someone who has already gone through this.

A long term solution is to do away with degrees and replace them with certification programs. If coders have taught themselves everything they need to know on their own they would take the test and prove their worthiness to be hired. This is common in other industries. It would allow for the adaptation to industry shifts.